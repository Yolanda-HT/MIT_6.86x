## Unit 3 )verview
At the end of this unit, you will be able to

- Implement a feedforward neural networks from scratch to perform image classification task.

- Write down the gradient of the loss function with respect to the weight parameters using back-propagation algorithm and use SGD to train neural networks.

- Understand that Recurrent Neural Networks (RNNs) and long short-term memory (LSTM) can be applied in modeling and generating sequences.

- Implement a Convolutional neural networks (CNNs) with machine learning packages.


# Introduction to Feedforward Neural Networks

## Objectives

At the end of this lecture, you will be able to

- Recognize different layers in a feedforward neural network and the number of units in each layer.

- Write down common activation functions such as the hyperbolic tangent function  tanh , and the rectified linear function (ReLU).

- Compute the output of a simple neural network possibly with hidden layers given the weights and activation functions .

- Determine whether data after transformation by some layers is linearly separable, draw decision boundaries given by the weight vectors and use them to help understand the behavior of the network.

## Motivation to Neural Networks

So far, the ways we have performed non-linear classification involve either first mapping  ğ‘¥  explicitly into some feature vectors  ğœ™(ğ‘¥),  whose coordinates involve non-linear functions of  ğ‘¥,  or in order to increase computational efficiency, rewriting the decision rule in terms of a chosen kernel, i.e. the dot product of feature vectors, and then using the training data to learn a transformed classification parameter.

However, in both cases, the feature vectors are chosen . They are not learned in order to improve performance of the classification problem at hand.

Neural networks, on the other hand, are models in which the *feature representation is learned jointly with the classifier* to improve classification performance.


## Neural Network Units
A neural network unit is a primitive neural network that consists of only the â€œinput layer", and an output layer with only one output. It is represented pictorially as follows:


A neural network unit computes a non-linear weighted combination of its input:

 	 ğ‘¦Ì‚  	 = 	 ğ‘“(ğ‘§)where ğ‘§=ğ‘¤0+âˆ‘ğ‘–=1ğ‘‘ğ‘¥ğ‘–ğ‘¤ğ‘– 	 	 
where  ğ‘¤ğ‘–  are numbers called **weights**,  ğ‘§  is a number and is the weighted sum of the inputs  ğ‘¥ğ‘–,  and  ğ‘“  is generally a non-linear function called the **activation function**.

The above equation in vector form is:

 	 ğ‘¦Ì‚  	 = 	 ğ‘“(ğ‘§)where ğ‘§=ğ‘¤0+ğ‘¥â‹…ğ‘¤, 	 	 
where  ğ‘¥=[ğ‘¥1,â€¦,ğ‘¥ğ‘‘]  and  ğ‘¤=[ğ‘¤1,â€¦,ğ‘¤ğ‘‘]ğ‘‡ .


**hyperbolic tangent function** is defined as
 	 tanh(ğ‘§) 	 = 	 ğ‘’ğ‘§âˆ’ğ‘’âˆ’ğ‘§ğ‘’ğ‘§+ğ‘’âˆ’ğ‘§=1âˆ’2ğ‘’2ğ‘§+1. 	


##  Introduction to Deep Neural Networks
A deep (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also hidden layers in between. For example, below is a deep feedfoward neural network of 2 hidden layers, with each hidden layer consisting of 5 units:


One of the main advantages of deep neural networks is that in many cases, they can learn to extract very complex and sophisticated features from just the raw features presented to them as their input. For instance, in the context of image recognition, neural networks can extract the features that differentiate a cat from a dog based only on the raw pixel data presented to them from images.

The initial few layers of a neural networks typically capture the simpler and smaller features whereas the later layers use information from these low-level features to identify more complex and sophisticated features. 	 

- Loosely motivated by biological neurons, networks
- Adjustable processing units (~ linear classifers)
- Highly parallel, typically organized in layers
- Deep = many transformations (layers) before output






