## Objectives

Feedforward Neural Networks, Back-propagation, and Stochastic Gradient Descent At the end of this lecture, you will be able to

- Write down **recursive relations** with **back-propagation** algorithm to compute the gradient of the loss function with respect to the weight parameters.

- Use the **stochastic descent algorithm** to train a feedforward neural network.

- Understand that it is not guaranteed to reach global (only local) optimum with SGD to minimize the training loss.

- Recognize when a network has **overcapacity**.


## Back propagatino

*[Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)*


Once we set up the architecture of our (feedforward) neural network, our goal will be to find weight parameters that minimize our loss function. We will use the stochastic gradient descent algorithm (which you learned in Lecture 4 and revisited in lecture 5) to carry out the optimization.

This involves computing the gradient of the loss function with respect to the weight parameters.

Since the loss function is a long chain of compositions of activation functions with the weight parameters entering at different stages, we will break down the computation of the gradient into different pieces via the chain rule; this way of computing the gradient is called the back-propagation algorithm.

In the following problems, we will explore the main step in the stochastic gradient descent algorithm for training the following simple neural network from the video:


This simple neural network is made up of  ğ¿ hidden layers, but each layer consists of only one unit, and each unit has activation function  ğ‘“ .
As usual,  ğ‘¥  is the input,  ğ‘§ğ‘–  is the weighted combination of the inputs to the  ğ‘–ğ‘¡â„  hidden layer. In this one-dimensional case, weighted combination reduces to products:

 	 ğ‘§1 	 = 	 ğ‘¥ğ‘¤1 	 	 
 	 for ğ‘–=2â€¦ğ¿:ğ‘§ğ‘– 	 = 	 ğ‘“ğ‘–âˆ’1ğ‘¤ğ‘–
 	 where ğ‘“ğ‘–âˆ’1=ğ‘“(ğ‘§ğ‘–âˆ’1). 	 	 
We will use the following loss function:

 	 îˆ¸(ğ‘¦,ğ‘“ğ¿)=(ğ‘¦âˆ’ğ‘“ğ¿)2 	 	 
where  ğ‘¦  is the true value, and  ğ‘“ğ¿  is the output of the neural network.